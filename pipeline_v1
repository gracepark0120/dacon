# using gracehpark:dacon/v1 docker file 
# app
# |--test_img
# |--train_img
# |--model.py
# |--train.py
# |--test.csv
# |--test.csv

import kfp
from kfp import dsl
from kfp.components import create_component_from_func, InputPath, OutputPath, func_to_container_op
from kfp.dsl.types import List
from functools import partial


# Define a component to train the model


def train_model(base_model_path: str, num_epoch: int) -> str:
     
    import pandas as pd
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    from torch.utils.data import Dataset, DataLoader
    import numpy as np
    import cv2

    def rle_decode(mask_rle, shape):
        s = mask_rle.split()
        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]
        starts -= 1
        ends = starts + lengths
        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)
        for lo, hi in zip(starts, ends):
            img[lo:hi] = 1
        return img.reshape(shape)

    class SatelliteDataset(Dataset):
        def __init__(self, csv_file, transform=None, infer=False):
            self.data = pd.read_csv(csv_file) 
            self.transform = transform
            self.infer = infer

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            img_path = self.data.iloc[idx, 1] 
            if './train_img' in img_path:
              new_path = img_path.replace('./train_img', './app/train_img')
            elif './test_img' in img_path:
              new_path = img_path.replace('./test_img', './app/test_img')

         #   image = cv2.imread(new_path)
            image = cv2.imread(img_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            if self.infer:
                if self.transform:
                    image = self.transform(image=image)['image']
                return image 

            mask_rle = self.data.iloc[idx, 2] 
            mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))
            if self.transform:
                augmented = self.transform(image=image, mask=mask)
                image = augmented['image']
                mask = augmented['mask']

            return image, mask

    train_transform = A.Compose(
            [
                A.OneOf(
                    [
                        A.RandomBrightness(p=1),
                        A.RandomBrightnessContrast(p=1),
                        A.Emboss(p=1),
                        A.RandomShadow(p=1),
                        A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),
                        A.NoOp(),

                    ],
                    p=1,
                ),
                A.OneOf(
                    [
                        A.Blur(p=1),
                        A.MotionBlur(p=1),
                    ],
                    p=0.6,
                ),
                A.OneOf(
                    [
                        A.NoOp(),
                        A.HorizontalFlip(p=0.5),
                        A.VerticalFlip(p=0.5),
                        A.ShiftScaleRotate(p=0.5),
                        A.Rotate(limit=90, p=1, border_mode=cv2.BORDER_REPLICATE),
                        A.RandomRotate90(p=1)
                    ],
                    p=1,
                ),
                A.ElasticTransform(),
                A.RandomCrop(224, 224),
                A.Normalize(),
                ToTensorV2(transpose_mask=True)
            ]
        )


    test_transform = A.Compose(
        [
            A.Resize(224, 224),
            A.Normalize(),
            ToTensorV2()
        ]
    )

    train_dataset = SatelliteDataset(csv_file='./app/train.csv', transform=train_transform)
    test_dataset = SatelliteDataset('./app/test.csv', transform=test_transform, infer=True)
    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)
    train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=8)


    import torch
    import argparse
    from tqdm import tqdm
    import numpy as np


    model.load_state_dict(torch.load(base_model_path))
    model.to(device)
    print("--------------------------------------------------")
    print(device)
    
    logs = []

    #criterion = HybridLoss()
    #criterion = DiceLoss()
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)

    torch.autograd.set_detect_anomaly(True)

    for epoch in range(num_epoch):
        model.train()
        epoch_loss = 0

        for images, masks in tqdm(train_dataloader):
            images = images.float().to(device)
            masks = masks.float().to(device)

            optimizer.zero_grad()
            out = model(images)

            loss = criterion(out, masks.unsqueeze(1))

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader)}')

        log_epoch = {'epoch': epoch+1,'train_loss': epoch_loss}
        logs.append(log_epoch)
        scheduler.step(epoch_loss)  
    
    output_model_path ="output_model.pth"
    torch.save(model.state_dict(), output_model_path)
    return output_model_path


# Define a component to preprocess data and create a model

def preprocess_and_create_model() -> str:
    # Your preprocessing and model creation code here
    import torch
    import torch.nn as nn
    import segmentation_models_pytorch as smp

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    class SMP(nn.Module):
        def __init__(self, encoder_name="resnet34", encoder_weights="imagenet", in_channels=3, classes=1):
            super().__init__()

            self.model = smp.UnetPlusPlus(
                encoder_name=encoder_name,
                encoder_weights=encoder_weights,
                in_channels=in_channels,
                classes=classes,
            )

        def forward(self, x):
            x = self.model(x)
            return x

    configs = {
        "model": {
            "encoder_name": "timm-regnety_320",
            "encoder_weights": "imagenet",
            "in_channels": 3,
            "classes": 1
        },
        "data": {
            "root": ".",
            "batch_size": 64
        },
    }

    model = SMP(
                encoder_name=configs["model"]["encoder_name"],
                encoder_weights=configs["model"]["encoder_weights"],
                in_channels=configs["model"]["in_channels"],
                classes=configs["model"]["classes"],
            )

    model = model.to(device)

    base_model_path = "/data/base_model.pth"
    torch.save(model.state_dict(), base_model_path)
    return base_model_path


@dsl.pipeline(
    name="Satellite Image Pipeline",
    description="Pipeline for satellite image processing",
)
def satellite_image_pipeline():
    data_op = dsl.VolumeOp(name="create-pvc",
                           resource_name="dacon-data-volume",
                           size="20Gi",
                           modes=dsl.VOLUME_MODE_RWO)
    # Preprocess data and create a model
    preprocess_op = func_to_container_op(preprocess_and_create_model, base_image="gracehpark/dacon:v1")
    train_op = func_to_container_op(train_model,packages_to_install=["requests_toolbelt", "urllib3"], base_image="gracehpark/dacon:v1")
    
    # Set up task dependencies

    
    step1 = preprocess_op().add_pvolumes({"/data": data_op.volume})
    step2 = train_op(step1.output,3).add_pvolumes({"/data": data_op.volume})

    

if __name__ == "__main__":
    import kfp
    from kfp.compiler import Compiler

    # Define the Docker image to use for the pipeline
  #  image_uri = "gracehpark/dacon:v1"

    # Compile and run the pipeline with the specified Docker image
    Compiler().compile(satellite_image_pipeline, "satellite_image_pipeline.yaml")

    """client = kfp.Client()
    client.create_run_from_pipeline_func(
            satellite_image_pipeline,
            arguments={
                "num_epoch": "practice1.pth",  # Example arguments
            },
            image=image_uri,
        )"""
